{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 1",
   "id": "99ff6ffb2099cf1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#~source: https: //en.wikipedia.org/wiki/Gradient_descent\n",
    "# code source: https://en.wikipedia.org/w/index.php?title=Gradient_descent&oldid=966271567\n",
    "\n",
    "# 初始值\n",
    "next_x = 6# We start the search at x = 6\n",
    "# 步长系数（学习率）\n",
    "gamma = 0.01# Step size multiplier\n",
    "# 提前停止（系数变化小于此值时停止）\n",
    "precision = 0.00001# Desired precision of result\n",
    "# 最大迭代次数\n",
    "max_iters = 10000# Maximum number of iterations\n",
    "\n",
    "# Derivative\n",
    "#function\n",
    "#求导\n",
    "def df(x):\n",
    "  return 4 * x ** 3 - 9 * x ** 2\n",
    "\n",
    "# 迭代\n",
    "for i in range(max_iters):\n",
    "    current_x = next_x\n",
    "    \n",
    "    #梯度下降\n",
    "    next_x = current_x - gamma * df(current_x)\n",
    "    print(i, next_x, df(current_x))\n",
    "\n",
    "    # 提前停止的判定，计算系数变化了多少，小于阈值后提前停止\n",
    "    step = next_x - current_x\n",
    "    if abs(step) <= precision:\n",
    "        break\n",
    "\n",
    "print(\"Minimum at \", next_x)\n",
    "\n",
    "# The output for the above will be something like \n",
    "# \"Minimum at 2.2499646074278457\""
   ],
   "id": "9c6f65ec7b78157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Source: https://github.com/mattnedrich/GradientDescentExample\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "# 计算均方误差 MSE\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # 实际的y减去 mx+b 的 y\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))"
   ],
   "id": "3670db91466a94b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Source: https://github.com/mattnedrich/GradientDescentExample\n",
    "\n",
    "# 梯度下降用于线性回归\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        \n",
    "        # 负梯度计算 （Loss Function 对 y_pre的导数，乘以 y_pred 对 m和b的导数\n",
    "        # 除以N是因为有N个样本，相当于取平均值\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]"
   ],
   "id": "486caad7e44f7d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 执行梯度下降的函数\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]"
   ],
   "id": "cbf9e2175aa38855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 用上面的代码合到一起，跑起来\n",
    "def run():\n",
    "    points = genfromtxt(\"data_linearreg.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print (\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print (\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)) )"
   ],
   "id": "98de73a2959b92e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#source: https://stackoverflow.com/questions/3949226/calculating-pearson-correlation-and-significance-in-python\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "# 计算 R Score\n",
    "def pcc(X, Y):\n",
    "   ''' Compute Pearson Correlation Coefficient. '''\n",
    "   # Normalise X and Y\n",
    "   X -= X.mean(0)\n",
    "   Y -= Y.mean(0)\n",
    "   # Standardise X and Y\n",
    "   X /= X.std(0)\n",
    "   Y /= Y.std(0)\n",
    "   # Compute mean product\n",
    "   return np.mean(X*Y)\n",
    " \n",
    "def average(x):\n",
    "    assert len(x) > 0\n",
    "    return float(sum(x)) / len(x)\n",
    "\n",
    "def pearson_def(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    n = len(x)\n",
    "    assert n > 0\n",
    "    avg_x = average(x)\n",
    "    avg_y = average(y)\n",
    "    diffprod = 0\n",
    "    xdiff2 = 0\n",
    "    ydiff2 = 0\n",
    "    for idx in range(n):\n",
    "        xdiff = x[idx] - avg_x\n",
    "        ydiff = y[idx] - avg_y\n",
    "        diffprod += xdiff * ydiff\n",
    "        xdiff2 += xdiff * xdiff\n",
    "        ydiff2 += ydiff * ydiff\n",
    "\n",
    "    # 协方差 除以 两个标准差的乘积\n",
    "    return diffprod / math.sqrt(xdiff2 * ydiff2)\n",
    "\n",
    "#main\n",
    "\n",
    "# Using it on a random example\n",
    "\n",
    "X = np.array([random() for x in range(100)])\n",
    "Y = np.array([random() for x in range(100)])\n",
    "\n",
    "\n",
    "# 两种等效的写法\n",
    "pcof = pcc(X, Y)\n",
    "print(pcof, ' is pcof')\n",
    " \n",
    "pcoftwo = pearson_def(X, Y)\n",
    "print(pcoftwo, ' is pcof second version')"
   ],
   "id": "2869b33e623bbfda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 2",
   "id": "c4eb0bd5842569c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Source: https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\n",
    "\n",
    "from math import exp\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "# 预测输出的概率 \n",
    "# 先计算 y_hat， 再使用 sigmoid函数转化为概率\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "# 梯度下降 更新参数 (权重 w 和 b) \n",
    "# 虽然标注了SGD，但这不是一个随机梯度下降，而是使用 SSE作为损失函数的GD： （全量的）批量梯度下降（如果使用MSE，求导时也要取均值，乘以1/N）\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    # 初始化系数\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "    # 迭代过程\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "        # 循环 遍历每一个sample\n",
    "\t\tfor row in train:\n",
    "            # 计算 预测的prop\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "            # 计算 伪残差\n",
    "\t\t\terror = row[-1] - yhat\n",
    "            # 计算 伪残差平方和，类似MSE\n",
    "\t\t\tsum_error += error**2\n",
    "            # 在原有的 系数基础上，加上 负梯度乘以学习率乘以 sigmoid 的导数\n",
    "            # 这里是在更新 b(bias) wx+b的b\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\t#print(row, yhat, error, sum_error)\n",
    "            # 这里是更新 w\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef\n",
    "\n",
    "# Calculate coefficients\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.3\n",
    "n_epoch = 10\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ],
   "id": "3f5436804f41688f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " \n",
    " # by R. Chandra\n",
    " #Source: https://github.com/rohitash-chandra/logistic_regression\n",
    "\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SIGMOID = 1\n",
    "STEP = 2\n",
    "LINEAR = 3\n",
    "\n",
    " \n",
    "random.seed()\n",
    "\n",
    "class logistic_regression:\n",
    "\n",
    "    # num_epocs 迭代次数\n",
    "    # train_data 训练集\n",
    "    # test_data 测试集\n",
    "    # num_features 特征数量\n",
    "    # learn_rate 学习率\n",
    "\tdef __init__(self, num_epocs, train_data, test_data, num_features, learn_rate):\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.test_data = test_data \n",
    "\t\tself.num_features = num_features\n",
    "        # 输出数据维度，列数减去输入特征数量\n",
    "\t\tself.num_outputs = self.train_data.shape[1] - num_features\n",
    "        # 样本量\n",
    "\t\tself.num_train = self.train_data.shape[0]\n",
    "\t\tself.w = np.random.uniform(-0.5, 0.5, num_features)  # in case one output class \n",
    "\t\tself.b = np.random.uniform(-0.5, 0.5, self.num_outputs) \n",
    "\t\tself.learn_rate = learn_rate\n",
    "\t\tself.max_epoch = num_epocs\n",
    "\t\tself.use_activation = SIGMOID #SIGMOID # 1 is  sigmoid , 2 is step, 3 is linear \n",
    "        # 用于记录偏导数\n",
    "\t\tself.out_delta = np.zeros(self.num_outputs)\n",
    "\n",
    "\t\tprint(self.w, ' self.w init') \n",
    "\t\tprint(self.b, ' self.b init') \n",
    "\t\tprint(self.out_delta, ' outdel init')\n",
    "\n",
    "\n",
    "    # 根据非激活值计算激活值\n",
    "\tdef activation_func(self,z_vec):\n",
    "\t\tif self.use_activation == SIGMOID:\n",
    "\t\t\ty =  1 / (1 + np.exp(z_vec)) # sigmoid/logistic\n",
    "\t\telif self.use_activation == STEP:\n",
    "\t\t\ty = (z_vec > 0).astype(int) # if greater than 0, use 1, else 0\n",
    "\t\t\t#https://stackoverflow.com/questions/32726701/convert-real-valued-numpy-array-to-binary-array-by-sign\n",
    "\t\telse:\n",
    "\t\t\ty = z_vec\n",
    "\t\treturn y\n",
    " \n",
    "    # 计算y的输出值\n",
    "\tdef predict(self, x_vec ): \n",
    "\t\tz_vec = x_vec.dot(self.w) - self.b \n",
    "\t\toutput = self.activation_func(z_vec) # Output  \n",
    "\t\treturn output\n",
    "\t\n",
    "\t# 计算梯度 （已修正）\n",
    "    # 预测值减实际值：梯度， 实际值减预测值：负梯度\n",
    "\tdef gradient(self, x_vec, output, actual):   \n",
    "\t\tif self.use_activation == SIGMOID :\n",
    "\t\t\tout_delta =   (output - actual)*(output*(1-output)) \n",
    "\t\telse: # for linear and step function  \n",
    "\t\t\tout_delta =   (output - actual) \n",
    "\t\treturn out_delta\n",
    "\n",
    "\t# 更新参数：这里有问题，正确写法是改成-=\n",
    "\tdef update(self, x_vec, output, actual):      \n",
    "\t\tself.w+= self.learn_rate *( x_vec *  self.out_delta)\n",
    "\t\tself.b+=  (1 * self.learn_rate * self.out_delta)\n",
    " \n",
    "\t# 计算 SSE\n",
    "    # 对于分类问题，这里的计算 将概率与实际值之差 视为伪残差\n",
    "\tdef squared_error(self, prediction, actual):\n",
    "\t\treturn  np.sum(np.square(prediction - actual))/prediction.shape[0]# to cater more in one output/class\n",
    " \n",
    "\n",
    "\t# 评估模型\n",
    "\tdef test_model(self, data, tolerance):  \n",
    "\n",
    "\t\tnum_instances = data.shape[0]\n",
    "\n",
    "\t\tclass_perf = 0\n",
    "\t\tsum_sqer = 0   \n",
    "        \n",
    "        #循环遍历每一个样本\n",
    "\t\tfor s in range(0, num_instances):\t\n",
    "\n",
    "\t\t\tinput_instance  =  self.train_data[s,0:self.num_features] \n",
    "\t\t\tactual  = self.train_data[s,self.num_features:]  \n",
    "\t\t\tprediction = self.predict(input_instance)\n",
    "\t\t\tsum_sqer += self.squared_error(prediction, actual)\n",
    "\n",
    "\t\t\t# 设置分类概率阈值\n",
    "\t\t\tpred_binary = np.where(prediction > (1 - tolerance), 1, 0)\n",
    "\n",
    "\t\t\tprint(s, actual, prediction, pred_binary, sum_sqer, ' s, actual, prediction, sum_sqer')\n",
    "\n",
    " \n",
    "\t\t\t# 预测正确 +1\n",
    "\t\t\tif( (actual==pred_binary).all()):\n",
    "\t\t\t\tclass_perf =  class_perf +1   \n",
    "\n",
    "\t\trmse = np.sqrt(sum_sqer/num_instances)\n",
    "\n",
    "\t\tpercentage_correct = float(class_perf/num_instances) * 100 \n",
    "\n",
    "\t\tprint(percentage_correct, rmse,  ' class_perf, rmse') \n",
    "\t\t# note RMSE is not a good measure for multi-class probs\n",
    "\n",
    "\t\treturn ( rmse, percentage_correct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\tdef SGD(self):   \n",
    "\t\t\n",
    "\t\t\tepoch = 0 \n",
    "\t\t\tshuffle = True\n",
    "\n",
    "\t\t\twhile  epoch < self.max_epoch:\n",
    "\t\t\t\tsum_sqer = 0\n",
    "\t\t\t\tfor s in range(0, self.num_train): \n",
    "\n",
    "\t\t\t\t\tif shuffle ==True:\n",
    "\t\t\t\t\t\ti = random.randint(0, self.num_train-1)\n",
    "\n",
    "\t\t\t\t\tinput_instance  =  self.train_data[i,0:self.num_features]  \n",
    "\t\t\t\t\tactual  = self.train_data[i,self.num_features:]  \n",
    "\t\t\t\t\tprediction = self.predict(input_instance) \n",
    "\t\t\t\t\tsum_sqer += self.squared_error(prediction, actual)\n",
    "\t\t\t\t\tself.out_delta = self.gradient( input_instance, prediction, actual)    # major difference when compared to GD\n",
    "\t\t\t\t\t#print(input_instance, prediction, actual, s, sum_sqer)\n",
    "                    \n",
    "                    # 使用梯度，更新参数\n",
    "\t\t\t\t\tself.update(input_instance, prediction, actual)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\tprint(epoch, sum_sqer, self.w, self.b)\n",
    "\t\t\t\tepoch=epoch+1  \n",
    "\n",
    "\t\t\trmse_train, train_perc = self.test_model(self.train_data, 0.3) \n",
    "\t\t\trmse_test =0\n",
    "\t\t\ttest_perc =0\n",
    "\t\t\t#rmse_test, test_perc = self.test_model(self.test_data, 0.3)\n",
    "  \n",
    "\t\t\treturn (train_perc, test_perc, rmse_train, rmse_test) \n",
    "\t\t\t\t\n",
    "\t# 梯度下降\n",
    "\tdef GD(self):   \n",
    "\t\t\n",
    "\t\t\tepoch = 0 \n",
    "\t\t\twhile  epoch < self.max_epoch:\n",
    "\t\t\t\tsum_sqer = 0\n",
    "\t\t\t\tfor s in range(0, self.num_train): \n",
    "\t\t\t\t\tinput_instance  =  self.train_data[s,0:self.num_features]  \n",
    "\t\t\t\t\tactual  = self.train_data[s,self.num_features:]   \n",
    "\t\t\t\t\tprediction = self.predict(input_instance) \n",
    "\t\t\t\t\tsum_sqer += self.squared_error(prediction, actual) \n",
    "\t\t\t\t\tself.out_delta+= self.gradient( input_instance, prediction, actual)    # this is major difference when compared with SGD\n",
    "\n",
    "\t\t\t\t\t#print(input_instance, prediction, actual, s, sum_sqer)\n",
    "                \n",
    "\t\t\t\t\t# 使用梯度，更新参数\n",
    "\t\t\t\t\tself.update(input_instance, prediction, actual)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\tprint(epoch, sum_sqer, self.w, self.b)\n",
    "\t\t\t\tepoch=epoch+1  \n",
    "\n",
    "\t\t\trmse_train, train_perc = self.test_model(self.train_data, 0.3) \n",
    "\t\t\trmse_test =0\n",
    "\t\t\ttest_perc =0\n",
    "\t\t\t#rmse_test, test_perc = self.test_model(self.test_data, 0.3)\n",
    "  \n",
    "\t\t\treturn (train_perc, test_perc, rmse_train, rmse_test) \n",
    "\t\t\t\t\n",
    "\t\n",
    " \n",
    "\n",
    "#------------------------------------------------------------------\n",
    "#MAIN\n",
    "\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "\trandom.seed()\n",
    "\t \n",
    "\n",
    "\t \n",
    "\tdataset = [[2.7810836,2.550537003,0],\n",
    "\t\t[1.465489372,2.362125076,0],\n",
    "\t\t[3.396561688,4.400293529,0],\n",
    "\t\t[1.38807019,1.850220317,0],\n",
    "\t\t[3.06407232,3.005305973,0],\n",
    "\t\t[7.627531214,2.759262235,1],\n",
    "\t\t[5.332441248,2.088626775,1],\n",
    "\t\t[6.922596716,1.77106367,1],\n",
    "\t\t[8.675418651,-0.242068655,1],\n",
    "\t\t[7.673756466,3.508563011,1]]\n",
    "\n",
    "\n",
    "\ttrain_data = np.asarray(dataset) # convert list data to numpy\n",
    "\ttest_data = train_data\n",
    "\n",
    "\t \n",
    "\n",
    "\tlearn_rate = 0.3\n",
    "\tnum_features = 2\n",
    "\tnum_epocs = 20\n",
    "\n",
    "\tprint(train_data)\n",
    "\t \n",
    "\n",
    "\tlreg = logistic_regression(num_epocs, train_data, test_data, num_features, learn_rate)\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.SGD()\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.GD() \n",
    "\t \n",
    "\n",
    "\t#-------------------------------\n",
    "\t#xor data\n",
    "\n",
    "\n",
    "\txor_dataset= [[0,0,0],\n",
    "\t\t[0,1,1],\n",
    "\t\t[1,0,1],\n",
    "\t\t[1,1,0] ]\n",
    "\n",
    "\txor_data = np.asarray(xor_dataset) # convert list data to numpy\n",
    "\n",
    "\n",
    "\n",
    "\tnum_epocs = 20\n",
    "\tlearn_rate = 0.9\n",
    "\tnum_features = 2\n",
    "\n",
    "\tlreg = logistic_regression(num_epocs, xor_data, xor_data, num_features, learn_rate)\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.SGD()\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.GD() \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "815e87e68ae74f64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T06:09:15.754779Z",
     "start_time": "2024-12-01T06:09:10.288042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "# 对数损失计算函数\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "# 下面是例子：用这个函数做了两个计算\n",
    "h= np.random.rand(5)\n",
    "print(h, ' h')\n",
    "y= np.random.rand(5)\n",
    "print(y, ' y')\n",
    "  \n",
    "log_loss = loss(h, y) # case of regression or prediction problem\n",
    "print(log_loss)\n",
    "\n",
    "y_ = np.ones(5)\n",
    "print(y_, ' y_')\n",
    "log_loss = loss(h, y_) # case of classification problem (assume class one 1s)\n",
    "print(log_loss)"
   ],
   "id": "9f01979d40f34663",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29929918 0.86508897 0.02141501 0.58179658 0.52617156]  h\n",
      "[0.67575181 0.12409989 0.26588349 0.61875227 0.17804302]  y\n",
      "1.0273292766693767\n",
      "[1. 1. 1. 1. 1.]  y_\n",
      "1.2757319854390659\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "93ced6440882d8e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
