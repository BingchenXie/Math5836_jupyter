{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 1",
   "id": "99ff6ffb2099cf1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#~source: https: //en.wikipedia.org/wiki/Gradient_descent\n",
    "# code source: https://en.wikipedia.org/w/index.php?title=Gradient_descent&oldid=966271567\n",
    "\n",
    "# 初始值\n",
    "next_x = 6# We start the search at x = 6\n",
    "# 步长系数（学习率）\n",
    "gamma = 0.01# Step size multiplier\n",
    "# 提前停止（系数变化小于此值时停止）\n",
    "precision = 0.00001# Desired precision of result\n",
    "# 最大迭代次数\n",
    "max_iters = 10000# Maximum number of iterations\n",
    "\n",
    "# Derivative\n",
    "#function\n",
    "#求导\n",
    "def df(x):\n",
    "  return 4 * x ** 3 - 9 * x ** 2\n",
    "\n",
    "# 迭代\n",
    "for i in range(max_iters):\n",
    "    current_x = next_x\n",
    "    \n",
    "    #梯度下降\n",
    "    next_x = current_x - gamma * df(current_x)\n",
    "    print(i, next_x, df(current_x))\n",
    "\n",
    "    # 提前停止的判定，计算系数变化了多少，小于阈值后提前停止\n",
    "    step = next_x - current_x\n",
    "    if abs(step) <= precision:\n",
    "        break\n",
    "\n",
    "print(\"Minimum at \", next_x)\n",
    "\n",
    "# The output for the above will be something like \n",
    "# \"Minimum at 2.2499646074278457\""
   ],
   "id": "9c6f65ec7b78157",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Source: https://github.com/mattnedrich/GradientDescentExample\n",
    "# y = mx + b\n",
    "# m is slope, b is y-intercept\n",
    "# 计算均方误差 MSE\n",
    "def compute_error_for_line_given_points(b, m, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        # 实际的y减去 mx+b 的 y\n",
    "        totalError += (y - (m * x + b)) ** 2\n",
    "    return totalError / float(len(points))"
   ],
   "id": "3670db91466a94b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Source: https://github.com/mattnedrich/GradientDescentExample\n",
    "\n",
    "# 梯度下降用于线性回归\n",
    "def step_gradient(b_current, m_current, points, learningRate):\n",
    "    b_gradient = 0\n",
    "    m_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        \n",
    "        # 负梯度计算 （Loss Function 对 y_pre的导数，乘以 y_pred 对 m和b的导数\n",
    "        # 除以N是因为有N个样本，相当于取平均值\n",
    "        b_gradient += -(2/N) * (y - ((m_current * x) + b_current))\n",
    "        m_gradient += -(2/N) * x * (y - ((m_current * x) + b_current))\n",
    "    new_b = b_current - (learningRate * b_gradient)\n",
    "    new_m = m_current - (learningRate * m_gradient)\n",
    "    return [new_b, new_m]"
   ],
   "id": "486caad7e44f7d97",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 执行梯度下降的函数\n",
    "def gradient_descent_runner(points, starting_b, starting_m, learning_rate, num_iterations):\n",
    "    b = starting_b\n",
    "    m = starting_m\n",
    "    for i in range(num_iterations):\n",
    "        b, m = step_gradient(b, m, array(points), learning_rate)\n",
    "    return [b, m]"
   ],
   "id": "cbf9e2175aa38855",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 用上面的代码合到一起，跑起来\n",
    "def run():\n",
    "    points = genfromtxt(\"data_linearreg.csv\", delimiter=\",\")\n",
    "    learning_rate = 0.0001\n",
    "    initial_b = 0 # initial y-intercept guess\n",
    "    initial_m = 0 # initial slope guess\n",
    "    num_iterations = 1000\n",
    "    print (\"Starting gradient descent at b = {0}, m = {1}, error = {2}\".format(initial_b, initial_m, compute_error_for_line_given_points(initial_b, initial_m, points)))\n",
    "    print (\"Running...\")\n",
    "    [b, m] = gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\n",
    "    print (\"After {0} iterations b = {1}, m = {2}, error = {3}\".format(num_iterations, b, m, compute_error_for_line_given_points(b, m, points)) )"
   ],
   "id": "98de73a2959b92e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#source: https://stackoverflow.com/questions/3949226/calculating-pearson-correlation-and-significance-in-python\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from random import random\n",
    "\n",
    "# 计算 R Score\n",
    "def pcc(X, Y):\n",
    "   ''' Compute Pearson Correlation Coefficient. '''\n",
    "   # Normalise X and Y\n",
    "   X -= X.mean(0)\n",
    "   Y -= Y.mean(0)\n",
    "   # Standardise X and Y\n",
    "   X /= X.std(0)\n",
    "   Y /= Y.std(0)\n",
    "   # Compute mean product\n",
    "   return np.mean(X*Y)\n",
    " \n",
    "def average(x):\n",
    "    assert len(x) > 0\n",
    "    return float(sum(x)) / len(x)\n",
    "\n",
    "def pearson_def(x, y):\n",
    "    assert len(x) == len(y)\n",
    "    n = len(x)\n",
    "    assert n > 0\n",
    "    avg_x = average(x)\n",
    "    avg_y = average(y)\n",
    "    diffprod = 0\n",
    "    xdiff2 = 0\n",
    "    ydiff2 = 0\n",
    "    for idx in range(n):\n",
    "        xdiff = x[idx] - avg_x\n",
    "        ydiff = y[idx] - avg_y\n",
    "        diffprod += xdiff * ydiff\n",
    "        xdiff2 += xdiff * xdiff\n",
    "        ydiff2 += ydiff * ydiff\n",
    "\n",
    "    # 协方差 除以 两个标准差的乘积\n",
    "    return diffprod / math.sqrt(xdiff2 * ydiff2)\n",
    "\n",
    "#main\n",
    "\n",
    "# Using it on a random example\n",
    "\n",
    "X = np.array([random() for x in range(100)])\n",
    "Y = np.array([random() for x in range(100)])\n",
    "\n",
    "\n",
    "# 两种等效的写法\n",
    "pcof = pcc(X, Y)\n",
    "print(pcof, ' is pcof')\n",
    " \n",
    "pcoftwo = pearson_def(X, Y)\n",
    "print(pcoftwo, ' is pcof second version')"
   ],
   "id": "2869b33e623bbfda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 2",
   "id": "c4eb0bd5842569c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Source: https://machinelearningmastery.com/implement-logistic-regression-stochastic-gradient-descent-scratch-python/\n",
    "\n",
    "from math import exp\n",
    "\n",
    "# Make a prediction with coefficients\n",
    "# 预测输出的概率 \n",
    "# 先计算 y_hat， 再使用 sigmoid函数转化为概率\n",
    "def predict(row, coefficients):\n",
    "\tyhat = coefficients[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tyhat += coefficients[i + 1] * row[i]\n",
    "\treturn 1.0 / (1.0 + exp(-yhat))\n",
    "\n",
    "# Estimate logistic regression coefficients using stochastic gradient descent\n",
    "# 梯度下降 更新参数 (权重 w 和 b) \n",
    "# 虽然标注了SGD，但这不是一个随机梯度下降，而是使用 SSE作为损失函数的GD： （全量的）批量梯度下降（如果使用MSE，求导时也要取均值，乘以1/N）\n",
    "def coefficients_sgd(train, l_rate, n_epoch):\n",
    "    # 初始化系数\n",
    "\tcoef = [0.0 for i in range(len(train[0]))]\n",
    "    # 迭代过程\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "        # 循环 遍历每一个sample\n",
    "\t\tfor row in train:\n",
    "            # 计算 预测的prop\n",
    "\t\t\tyhat = predict(row, coef)\n",
    "            # 计算 伪残差\n",
    "\t\t\terror = row[-1] - yhat\n",
    "            # 计算 伪残差平方和，类似MSE\n",
    "\t\t\tsum_error += error**2\n",
    "            # 在原有的 系数基础上，加上 负梯度乘以学习率乘以 sigmoid 的导数\n",
    "            # 这里是在更新 b(bias) wx+b的b\n",
    "\t\t\tcoef[0] = coef[0] + l_rate * error * yhat * (1.0 - yhat)\n",
    "\t\t\t#print(row, yhat, error, sum_error)\n",
    "            # 这里是更新 w\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tcoef[i + 1] = coef[i + 1] + l_rate * error * yhat * (1.0 - yhat) * row[i]\n",
    "\t\tprint('>epoch=%d, lrate=%.3f, error=%.3f' % (epoch, l_rate, sum_error))\n",
    "\treturn coef\n",
    "\n",
    "# Calculate coefficients\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "l_rate = 0.3\n",
    "n_epoch = 10\n",
    "coef = coefficients_sgd(dataset, l_rate, n_epoch)\n",
    "print(coef)"
   ],
   "id": "3f5436804f41688f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    " \n",
    " # by R. Chandra\n",
    " #Source: https://github.com/rohitash-chandra/logistic_regression\n",
    "\n",
    "from math import exp\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SIGMOID = 1\n",
    "STEP = 2\n",
    "LINEAR = 3\n",
    "\n",
    " \n",
    "random.seed()\n",
    "\n",
    "class logistic_regression:\n",
    "\n",
    "    # num_epocs 迭代次数\n",
    "    # train_data 训练集\n",
    "    # test_data 测试集\n",
    "    # num_features 特征数量\n",
    "    # learn_rate 学习率\n",
    "\tdef __init__(self, num_epocs, train_data, test_data, num_features, learn_rate):\n",
    "\t\tself.train_data = train_data\n",
    "\t\tself.test_data = test_data \n",
    "\t\tself.num_features = num_features\n",
    "        # 输出数据维度，列数减去输入特征数量\n",
    "\t\tself.num_outputs = self.train_data.shape[1] - num_features\n",
    "        # 样本量\n",
    "\t\tself.num_train = self.train_data.shape[0]\n",
    "\t\tself.w = np.random.uniform(-0.5, 0.5, num_features)  # in case one output class \n",
    "\t\tself.b = np.random.uniform(-0.5, 0.5, self.num_outputs) \n",
    "\t\tself.learn_rate = learn_rate\n",
    "\t\tself.max_epoch = num_epocs\n",
    "\t\tself.use_activation = SIGMOID #SIGMOID # 1 is  sigmoid , 2 is step, 3 is linear \n",
    "        # 用于记录偏导数\n",
    "\t\tself.out_delta = np.zeros(self.num_outputs)\n",
    "\n",
    "\t\tprint(self.w, ' self.w init') \n",
    "\t\tprint(self.b, ' self.b init') \n",
    "\t\tprint(self.out_delta, ' outdel init')\n",
    "\n",
    "\n",
    "    # 根据非激活值计算激活值\n",
    "\tdef activation_func(self,z_vec):\n",
    "\t\tif self.use_activation == SIGMOID:\n",
    "\t\t\ty =  1 / (1 + np.exp(z_vec)) # sigmoid/logistic\n",
    "\t\telif self.use_activation == STEP:\n",
    "\t\t\ty = (z_vec > 0).astype(int) # if greater than 0, use 1, else 0\n",
    "\t\t\t#https://stackoverflow.com/questions/32726701/convert-real-valued-numpy-array-to-binary-array-by-sign\n",
    "\t\telse:\n",
    "\t\t\ty = z_vec\n",
    "\t\treturn y\n",
    " \n",
    "    # 计算y的输出值\n",
    "\tdef predict(self, x_vec ): \n",
    "\t\tz_vec = x_vec.dot(self.w) - self.b \n",
    "\t\toutput = self.activation_func(z_vec) # Output  \n",
    "\t\treturn output\n",
    "\t\n",
    "\t# 计算梯度 （已修正）\n",
    "    # 预测值减实际值：梯度， 实际值减预测值：负梯度\n",
    "\tdef gradient(self, x_vec, output, actual):   \n",
    "\t\tif self.use_activation == SIGMOID :\n",
    "\t\t\tout_delta =   (output - actual)*(output*(1-output)) \n",
    "\t\telse: # for linear and step function  \n",
    "\t\t\tout_delta =   (output - actual) \n",
    "\t\treturn out_delta\n",
    "\n",
    "\t# 更新参数：这里有问题，正确写法是改成-=\n",
    "\tdef update(self, x_vec, output, actual):      \n",
    "\t\tself.w+= self.learn_rate *( x_vec *  self.out_delta)\n",
    "\t\tself.b+=  (1 * self.learn_rate * self.out_delta)\n",
    " \n",
    "\t# 计算 SSE\n",
    "    # 对于分类问题，这里的计算 将概率与实际值之差 视为伪残差\n",
    "\tdef squared_error(self, prediction, actual):\n",
    "\t\treturn  np.sum(np.square(prediction - actual))/prediction.shape[0]# to cater more in one output/class\n",
    " \n",
    "\n",
    "\t# 评估模型\n",
    "\tdef test_model(self, data, tolerance):  \n",
    "\n",
    "\t\tnum_instances = data.shape[0]\n",
    "\n",
    "\t\tclass_perf = 0\n",
    "\t\tsum_sqer = 0   \n",
    "        \n",
    "        #循环遍历每一个样本\n",
    "\t\tfor s in range(0, num_instances):\t\n",
    "\n",
    "\t\t\tinput_instance  =  self.train_data[s,0:self.num_features] \n",
    "\t\t\tactual  = self.train_data[s,self.num_features:]  \n",
    "\t\t\tprediction = self.predict(input_instance)\n",
    "\t\t\tsum_sqer += self.squared_error(prediction, actual)\n",
    "\n",
    "\t\t\t# 设置分类概率阈值\n",
    "\t\t\tpred_binary = np.where(prediction > (1 - tolerance), 1, 0)\n",
    "\n",
    "\t\t\tprint(s, actual, prediction, pred_binary, sum_sqer, ' s, actual, prediction, sum_sqer')\n",
    "\n",
    " \n",
    "\t\t\t# 预测正确 +1\n",
    "\t\t\tif( (actual==pred_binary).all()):\n",
    "\t\t\t\tclass_perf =  class_perf +1   \n",
    "\n",
    "\t\trmse = np.sqrt(sum_sqer/num_instances)\n",
    "\n",
    "\t\tpercentage_correct = float(class_perf/num_instances) * 100 \n",
    "\n",
    "\t\tprint(percentage_correct, rmse,  ' class_perf, rmse') \n",
    "\t\t# note RMSE is not a good measure for multi-class probs\n",
    "\n",
    "\t\treturn ( rmse, percentage_correct)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\tdef SGD(self):   \n",
    "\t\t\n",
    "\t\t\tepoch = 0 \n",
    "\t\t\tshuffle = True\n",
    "\n",
    "\t\t\twhile  epoch < self.max_epoch:\n",
    "\t\t\t\tsum_sqer = 0\n",
    "\t\t\t\tfor s in range(0, self.num_train): \n",
    "\n",
    "\t\t\t\t\tif shuffle ==True:\n",
    "\t\t\t\t\t\ti = random.randint(0, self.num_train-1)\n",
    "\n",
    "\t\t\t\t\tinput_instance  =  self.train_data[i,0:self.num_features]  \n",
    "\t\t\t\t\tactual  = self.train_data[i,self.num_features:]  \n",
    "\t\t\t\t\tprediction = self.predict(input_instance) \n",
    "\t\t\t\t\tsum_sqer += self.squared_error(prediction, actual)\n",
    "\t\t\t\t\tself.out_delta = self.gradient( input_instance, prediction, actual)    # major difference when compared to GD\n",
    "\t\t\t\t\t#print(input_instance, prediction, actual, s, sum_sqer)\n",
    "                    \n",
    "                    # 使用梯度，更新参数\n",
    "\t\t\t\t\tself.update(input_instance, prediction, actual)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\tprint(epoch, sum_sqer, self.w, self.b)\n",
    "\t\t\t\tepoch=epoch+1  \n",
    "\n",
    "\t\t\trmse_train, train_perc = self.test_model(self.train_data, 0.3) \n",
    "\t\t\trmse_test =0\n",
    "\t\t\ttest_perc =0\n",
    "\t\t\t#rmse_test, test_perc = self.test_model(self.test_data, 0.3)\n",
    "  \n",
    "\t\t\treturn (train_perc, test_perc, rmse_train, rmse_test) \n",
    "\t\t\t\t\n",
    "\t# 梯度下降\n",
    "\tdef GD(self):   \n",
    "\t\t\n",
    "\t\t\tepoch = 0 \n",
    "\t\t\twhile  epoch < self.max_epoch:\n",
    "\t\t\t\tsum_sqer = 0\n",
    "\t\t\t\tfor s in range(0, self.num_train): \n",
    "\t\t\t\t\tinput_instance  =  self.train_data[s,0:self.num_features]  \n",
    "\t\t\t\t\tactual  = self.train_data[s,self.num_features:]   \n",
    "\t\t\t\t\tprediction = self.predict(input_instance) \n",
    "\t\t\t\t\tsum_sqer += self.squared_error(prediction, actual) \n",
    "\t\t\t\t\tself.out_delta+= self.gradient( input_instance, prediction, actual)    # this is major difference when compared with SGD\n",
    "\n",
    "\t\t\t\t\t#print(input_instance, prediction, actual, s, sum_sqer)\n",
    "                \n",
    "\t\t\t\t\t# 使用梯度，更新参数\n",
    "\t\t\t\t\tself.update(input_instance, prediction, actual)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\t\tprint(epoch, sum_sqer, self.w, self.b)\n",
    "\t\t\t\tepoch=epoch+1  \n",
    "\n",
    "\t\t\trmse_train, train_perc = self.test_model(self.train_data, 0.3) \n",
    "\t\t\trmse_test =0\n",
    "\t\t\ttest_perc =0\n",
    "\t\t\t#rmse_test, test_perc = self.test_model(self.test_data, 0.3)\n",
    "  \n",
    "\t\t\treturn (train_perc, test_perc, rmse_train, rmse_test) \n",
    "\t\t\t\t\n",
    "\t\n",
    " \n",
    "\n",
    "#------------------------------------------------------------------\n",
    "#MAIN\n",
    "\n",
    "\n",
    "\n",
    "def main(): \n",
    "\n",
    "\trandom.seed()\n",
    "\t \n",
    "\n",
    "\t \n",
    "\tdataset = [[2.7810836,2.550537003,0],\n",
    "\t\t[1.465489372,2.362125076,0],\n",
    "\t\t[3.396561688,4.400293529,0],\n",
    "\t\t[1.38807019,1.850220317,0],\n",
    "\t\t[3.06407232,3.005305973,0],\n",
    "\t\t[7.627531214,2.759262235,1],\n",
    "\t\t[5.332441248,2.088626775,1],\n",
    "\t\t[6.922596716,1.77106367,1],\n",
    "\t\t[8.675418651,-0.242068655,1],\n",
    "\t\t[7.673756466,3.508563011,1]]\n",
    "\n",
    "\n",
    "\ttrain_data = np.asarray(dataset) # convert list data to numpy\n",
    "\ttest_data = train_data\n",
    "\n",
    "\t \n",
    "\n",
    "\tlearn_rate = 0.3\n",
    "\tnum_features = 2\n",
    "\tnum_epocs = 20\n",
    "\n",
    "\tprint(train_data)\n",
    "\t \n",
    "\n",
    "\tlreg = logistic_regression(num_epocs, train_data, test_data, num_features, learn_rate)\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.SGD()\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.GD() \n",
    "\t \n",
    "\n",
    "\t#-------------------------------\n",
    "\t#xor data\n",
    "\n",
    "\n",
    "\txor_dataset= [[0,0,0],\n",
    "\t\t[0,1,1],\n",
    "\t\t[1,0,1],\n",
    "\t\t[1,1,0] ]\n",
    "\n",
    "\txor_data = np.asarray(xor_dataset) # convert list data to numpy\n",
    "\n",
    "\n",
    "\n",
    "\tnum_epocs = 20\n",
    "\tlearn_rate = 0.9\n",
    "\tnum_features = 2\n",
    "\n",
    "\tlreg = logistic_regression(num_epocs, xor_data, xor_data, num_features, learn_rate)\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.SGD()\n",
    "\t(train_perc, test_perc, rmse_train, rmse_test) = lreg.GD() \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": main()"
   ],
   "id": "815e87e68ae74f64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LogLoss计算\n",
    "\n",
    "import numpy as np\n",
    "# 对数损失计算函数\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "# 下面是例子：用这个函数做了两个计算\n",
    "h= np.random.rand(5)\n",
    "print(h, ' h')\n",
    "y= np.random.rand(5)\n",
    "print(y, ' y')\n",
    "  \n",
    "log_loss = loss(h, y) # case of regression or prediction problem\n",
    "print(log_loss)\n",
    "\n",
    "y_ = np.ones(5)\n",
    "print(y_, ' y_')\n",
    "log_loss = loss(h, y_) # case of classification problem (assume class one 1s)\n",
    "print(log_loss)"
   ],
   "id": "9f01979d40f34663",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 9",
   "id": "b22c4bbdbb34e7d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# SVD数学计算代码\n",
    "\n",
    "from numpy import array\n",
    "from numpy import diag\n",
    "from numpy import zeros\n",
    "from scipy.linalg import svd\n",
    "# define a matrix\n",
    "A = array([\n",
    "\t[1,2,3,4,5,6,7,8,9,10],\n",
    "\t[11,12,13,14,15,16,17,18,19,20],\n",
    "\t[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# Singular-value decomposition\n",
    "# A = USV (S为sigma,但是表示为一个向量而非矩阵，包含了sigma矩阵的全部对角元素)\n",
    "U, s, VT = svd(A)\n",
    "\n",
    "# 使用对角元素向量s构建sigma矩阵\n",
    "# create m x n Sigma matrix\n",
    "Sigma = zeros((A.shape[0], A.shape[1]))\n",
    "# populate Sigma with n x n diagonal matrix\n",
    "Sigma[:A.shape[0], :A.shape[0]] = diag(s)\n",
    "\n",
    "# 特征选择，选择sigma的前两列，V的前两行\n",
    "# select\n",
    "n_elements = 2\n",
    "Sigma = Sigma[:, :n_elements]\n",
    "VT = VT[:n_elements, :]\n",
    "\n",
    "# 点乘，B是原矩阵A的一个低秩近似\n",
    "# reconstruct\n",
    "B = U.dot(Sigma.dot(VT))\n",
    "print(B)\n",
    "\n",
    "# SVD分解结果为T，有两种计算方法，结果一致\n",
    "# transform\n",
    "T = U.dot(Sigma)\n",
    "print(T)\n",
    "T = A.dot(VT.T)\n",
    "print(T)\n",
    "\n",
    "#Running the example first prints the defined matrix then the reconstructed\n",
    "# approximation, followed by two equivalent transforms of the original matrix.\n",
    "\n",
    "print(' Truncated SVD')\n",
    "#Truncated SVD\n",
    "\n",
    "#The TruncatedSVD class can be created in which you must specify the number\n",
    "# of desirable features or components to select, e.g. 2. Once created, \n",
    "#you can fit the transform (e.g. calculate V^Tk) by calling the fit() \n",
    "#function, then apply it to the original matrix by calling the transform() \n",
    "#function. The result is the transform of A called T above.\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "# define array\n",
    "A = array([\n",
    "\t[1,2,3,4,5,6,7,8,9,10],\n",
    "\t[11,12,13,14,15,16,17,18,19,20],\n",
    "\t[21,22,23,24,25,26,27,28,29,30]])\n",
    "print(A)\n",
    "# svd\n",
    "svd = TruncatedSVD(n_components=2)\n",
    "svd.fit(A)\n",
    "result = svd.transform(A)\n",
    "print(result)"
   ],
   "id": "be6a503b1354f3dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 同理，只是这段代码用了np\n",
    "import numpy as np\n",
    "U, s, Vt = np.linalg.svd(A)\n",
    "W2 = Vt.T[:, :2]\n",
    "X2D = A.dot(W2)"
   ],
   "id": "3bb848a959e11f90",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 增量PCA\n",
    "\n",
    "# Authors: Kyle Kastner\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "\n",
    "# 加载数据\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "n_components = 2\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=10)\n",
    "X_ipca = ipca.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(X_ipca, ' X_ipca')\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for X_transformed, title in [(X_ipca, \"Incremental PCA\"), (X_pca, \"PCA\")]:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],\n",
    "                    color=color, lw=2, label=target_name)\n",
    "\n",
    "    if True or \"Incremental\" in title:\n",
    "        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n",
    "        plt.title(title + \" of iris dataset\\nMean absolute unsigned error \"\n",
    "                  \"%.6f\" % err)\n",
    "    else:\n",
    "        plt.title(title + \" of iris dataset\")\n",
    "    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "    plt.axis([-4, 4, -1.5, 1.5])\n",
    "\n",
    "plt.savefig('ipca.png')\n"
   ],
   "id": "24ba456647a64fff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#K-Means 聚类算法\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "#style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k=3, tol=0.01, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        self.centroids = {}\n",
    "\n",
    "    def fit(self,data):\n",
    "\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i]\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            print(i, self.centroids, ' centroids')\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "            \n",
    "            print(self.classifications, i, ' * ')\n",
    "\n",
    "            for featureset in data:\n",
    "                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids] # check distance of every data point to every centroid\n",
    "                print(distances, featureset, ' dist - featureset')\n",
    "                classification = distances.index(min(distances)) # get index of centroid that best suits the data point\n",
    "                self.classifications[classification].append(featureset) # assign the centroid id to the data point\n",
    "            \n",
    "            print(self.classifications, i, ' + ')\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "\n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification],axis=0)\n",
    "\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids: # update centroid position\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:\n",
    "                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "        return self.centroids\n",
    "\n",
    "    def predict(self,data):\n",
    "        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "\n",
    "#main\n",
    "\n",
    "X = np.array([[1, 2],\n",
    "              [1.5, 1.8],\n",
    "              [5, 8 ],\n",
    "              [8, 8],\n",
    "              [1, 0.6],\n",
    "              [9,11],\n",
    "              [1,3],\n",
    "              [8,9],\n",
    "              [0,3],\n",
    "              [5,4],\n",
    "              [6,4],])\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], s=150)\n",
    "plt.savefig('data.png')\n",
    "print(X, ' data')\n",
    "\n",
    "colors = 10*[\"g\",\"r\",\"c\",\"b\",\"k\"]\n",
    "clf = K_Means()\n",
    "x = clf.fit(X)\n",
    "print(x, 'final centroids')\n",
    "\n",
    "for centroid in clf.centroids:\n",
    "    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],\n",
    "                marker=\"o\", color=\"k\", s=150, linewidths=5)\n",
    "\n",
    "for classification in clf.classifications:\n",
    "    color = colors[classification]\n",
    "    for featureset in clf.classifications[classification]:\n",
    "        plt.scatter(featureset[0], featureset[1], marker=\"x\", color=color, s=150, linewidths=5)\n",
    "\n",
    "# in case we wish to test if data belongs to a cluster\n",
    "\n",
    "##unknowns = np.array([[1,3],\n",
    "##                     [8,9],\n",
    "##                     [0,3],\n",
    "##                     [5,4],\n",
    "##                     [6,4],])\n",
    "##\n",
    "##for unknown in unknowns:\n",
    "##    classification = clf.predict(unknown)\n",
    "##    plt.scatter(unknown[0], unknown[1], marker=\"*\", color=colors[classification], s=150, linewidths=5)\n",
    "##\n",
    "\n",
    "plt.savefig('kmeans.png')"
   ],
   "id": "ded843b9a4cdd29e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 软聚类代码\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "# Let's start by generating some blobs ---\n",
    "blob_centers = np.array(\n",
    "    [[ 0.2,  2.3],\n",
    "     [-1.5 ,  2.3],\n",
    "     [-2.8,  1.8],\n",
    "     [-2.8,  2.8],\n",
    "     [-2.8,  1.3]])\n",
    "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
    "X, y = make_blobs(n_samples=2000, centers=blob_centers,\n",
    "                  cluster_std=blob_std, random_state=7)\n",
    "# ------- end of generating blobs ----- \n",
    "k = 5\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "# The following assigns new instances to the cluster whose centroid is closest\n",
    "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n",
    "\n",
    "print('kmeans.transform(X_new):')\n",
    "print(kmeans.transform(X_new))\n",
    "\n"
   ],
   "id": "1bc91a220a5dbd8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
